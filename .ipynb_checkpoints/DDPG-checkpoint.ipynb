{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Deterministic Policy Gradients (DDPG)\n",
    "---\n",
    "In this notebook, we train DDPG with OpenAI Gym's BipedalWalker-v2 environment.\n",
    "\n",
    "### 1. Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from ddpg_agent import Agent\n",
    "\n",
    "# imports for rendering outputs in Jupyter.\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Instantiate the Environment and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/parminder0407/anaconda3/envs/cv3/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('BipedalWalker-v2')\n",
    "env.seed(10)\n",
    "agent = Agent(state_size=env.observation_space.shape[0], action_size=env.action_space.shape[0], random_seed=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action space: Box(4,) ... state space: Box(24,)\n"
     ]
    }
   ],
   "source": [
    "print (\"action space: {} ... state space: {}\".format(env.action_space,env.observation_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 1., 1., 1.], dtype=float32),\n",
       " array([-1., -1., -1., -1.], dtype=float32))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.high, env.action_space.low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0 ... state: [ 2.74737482e-03 -1.79915968e-05  1.39956169e-03 -1.59998775e-02] ... action: [0.09762701 0.43037874 0.20552675 0.08976637] ... reward: -0.02 ... done: False\n",
      "\n",
      "step: 1 ... state: [ 0.00245546 -0.00693401  0.00706781  0.01954999] ... action: [-0.1526904   0.29178822 -0.12482557  0.78354603] ... reward: -0.04 ... done: False\n",
      "\n",
      "step: 2 ... state: [0.0025492  0.00254582 0.00921938 0.0067264 ] ... action: [ 0.92732555 -0.23311697  0.5834501   0.05778984] ... reward: -0.18 ... done: False\n",
      "\n",
      "step: 3 ... state: [-0.02497778 -0.05565673 -0.03224418 -0.01146388] ... action: [ 0.13608912  0.85119325 -0.85792786 -0.8257414 ] ... reward: -0.15 ... done: False\n",
      "\n",
      "step: 4 ... state: [-0.03810721 -0.02684057 -0.01018923 -0.00028272] ... action: [-0.9595632   0.6652397   0.5563135   0.74002427] ... reward: -0.08 ... done: False\n",
      "\n",
      "step: 5 ... state: [-0.04034055 -0.00448921  0.02451716  0.07418973] ... action: [ 0.9572367   0.59831715 -0.07704128  0.56105834] ... reward: -0.13 ... done: False\n",
      "\n",
      "step: 6 ... state: [-0.05752897 -0.03441593  0.01659381  0.06765851] ... action: [-0.76345116  0.27984205 -0.71329343  0.88933784] ... reward: 0.01 ... done: False\n",
      "\n",
      "step: 7 ... state: [-0.04742908  0.02015501  0.05315211  0.0822543 ] ... action: [ 0.04369664 -0.17067613 -0.47088876  0.5484674 ] ... reward: 0.07 ... done: False\n",
      "\n",
      "step: 8 ... state: [-0.0325623   0.02969866  0.04517179  0.10635004] ... action: [-0.08769934  0.1368679  -0.9624204   0.23527099] ... reward: 0.08 ... done: False\n",
      "\n",
      "step: 9 ... state: [-0.0164213   0.03227819  0.05401522  0.09184275] ... action: [0.22419144 0.23386799 0.8874962  0.3636406 ] ... reward: -0.05 ... done: False\n",
      "\n",
      "step: 10 ... state: [-0.02086555 -0.00885637  0.03070331  0.07382854] ... action: [-0.2809842  -0.12593609  0.3952624  -0.8795491 ] ... reward: -0.08 ... done: False\n",
      "\n",
      "step: 11 ... state: [-0.02890876 -0.01608941  0.01602553  0.05242187] ... action: [ 0.33353344  0.34127575 -0.5792349  -0.7421474 ] ... reward: -0.06 ... done: False\n",
      "\n",
      "step: 12 ... state: [-0.03270517 -0.00761539  0.01862116  0.0361641 ] ... action: [-0.3691433  -0.27257845  0.14039354 -0.12279698] ... reward: -0.01 ... done: False\n",
      "\n",
      "step: 13 ... state: [-0.0328809  -0.0003912   0.01835848  0.01785957] ... action: [ 0.9767477  -0.79591036 -0.5822465  -0.677381  ] ... reward: -0.11 ... done: False\n",
      "\n",
      "step: 14 ... state: [-0.04027299 -0.01480291  0.01064349  0.00444693] ... action: [ 0.30621666 -0.4934168  -0.06737845 -0.5111488 ] ... reward: -0.09 ... done: False\n",
      "\n",
      "step: 15 ... state: [-0.0520908  -0.02366338  0.0055403  -0.0122085 ] ... action: [-0.68206084 -0.7792497   0.31265917 -0.7236341 ] ... reward: -0.09 ... done: False\n",
      "\n",
      "step: 16 ... state: [-0.05716193 -0.01018547  0.01270016 -0.03285013] ... action: [-0.6068353  -0.26254967  0.6419865  -0.80579746] ... reward: -0.09 ... done: False\n",
      "\n",
      "step: 17 ... state: [-0.06298675 -0.01171623  0.01111755 -0.05335963] ... action: [ 0.6758898  -0.8078032   0.95291895 -0.0626976 ] ... reward: -0.16 ... done: False\n",
      "\n",
      "step: 18 ... state: [-0.08038425 -0.03488452 -0.00221445 -0.06914218] ... action: [ 0.9535222   0.20969103  0.47852716 -0.9216244 ] ... reward: -0.20 ... done: False\n",
      "\n",
      "step: 19 ... state: [-0.10490359 -0.04913311 -0.00853194 -0.08341329] ... action: [-0.43438607 -0.7596069  -0.4077196  -0.7625446 ] ... reward: -0.10 ... done: False\n",
      "\n",
      "step: 20 ... state: [-0.113897   -0.01805255  0.0087796  -0.10238965] ... action: [-0.36403364 -0.17147401 -0.871705    0.38494423] ... reward: 0.03 ... done: False\n",
      "\n",
      "step: 21 ... state: [-0.10315292  0.02139277  0.03698362 -0.09916357] ... action: [ 0.13320291 -0.46922103  0.04649611 -0.812119  ] ... reward: 0.01 ... done: False\n",
      "\n",
      "step: 22 ... state: [-0.09629431  0.01359365  0.02757021 -0.13425879] ... action: [ 0.15189299  0.8585924  -0.36286208  0.33482075] ... reward: 0.04 ... done: False\n",
      "\n",
      "step: 23 ... state: [-0.08601809  0.02049692  0.04955736 -0.12029821] ... action: [-0.7364043   0.4326544  -0.42118782 -0.6336173 ] ... reward: 0.11 ... done: False\n",
      "\n",
      "step: 24 ... state: [-0.06142445  0.04904678  0.06220779 -0.15277295] ... action: [ 0.17302588 -0.9597849   0.65788007 -0.99060905] ... reward: -0.02 ... done: False\n",
      "\n",
      "step: 25 ... state: [-0.05441392  0.01397751  0.02673969 -0.1835519 ] ... action: [ 0.35563308 -0.45998406  0.47038805  0.9243771 ] ... reward: -0.09 ... done: False\n",
      "\n",
      "step: 26 ... state: [-0.06173464 -0.01469692  0.01668382 -0.18109945] ... action: [-0.50249374  0.15231466  0.18408386  0.14450382] ... reward: -0.00 ... done: False\n",
      "\n",
      "step: 27 ... state: [-0.06193202 -0.00044497  0.03370081 -0.19082911] ... action: [-0.55383676  0.905498   -0.10574924  0.69281733] ... reward: 0.03 ... done: False\n",
      "\n",
      "step: 28 ... state: [-0.051424    0.02098587  0.05230323 -0.20379299] ... action: [ 0.39895856 -0.4051261   0.62759566 -0.20698851] ... reward: -0.10 ... done: False\n",
      "\n",
      "step: 29 ... state: [-0.06299262 -0.02316376  0.00516007 -0.23512314] ... action: [0.7622064  0.16254574 0.7634707  0.38506317] ... reward: -0.20 ... done: False\n",
      "\n",
      "step: 30 ... state: [-0.09091111 -0.05587644 -0.00822925 -0.24260347] ... action: [0.45050856 0.00264876 0.91216725 0.2879804 ] ... reward: -0.21 ... done: False\n",
      "\n",
      "step: 31 ... state: [-0.12328134 -0.06480013 -0.00894084 -0.25487295] ... action: [-0.1522899   0.21278642 -0.9616136  -0.39685038] ... reward: -0.08 ... done: False\n",
      "\n",
      "step: 32 ... state: [-0.13359198 -0.02068608  0.02728035 -0.26458939] ... action: [ 0.32034707 -0.41984478  0.23603086 -0.1424626 ] ... reward: -0.14 ... done: False\n",
      "\n",
      "step: 33 ... state: [-0.15555918 -0.04397695  0.00324959 -0.29029831] ... action: [-0.7290519  -0.40343535  0.13992982  0.18174553] ... reward: -0.08 ... done: False\n",
      "\n",
      "step: 34 ... state: [-0.16584955 -0.02060399  0.02321209 -0.29936678] ... action: [ 0.1486505   0.30640164  0.30420655 -0.13716313] ... reward: -0.10 ... done: False\n",
      "\n",
      "step: 35 ... state: [-0.18408139 -0.03649281  0.01837843 -0.30734798] ... action: [ 0.7930932  -0.26487625 -0.12827015  0.78384674] ... reward: 0.01 ... done: False\n",
      "\n",
      "step: 36 ... state: [-0.19805005 -0.03132685  0.164157   -0.11710586] ... action: [ 0.61238796  0.40777716 -0.79954624  0.83896524] ... reward: 0.09 ... done: False\n",
      "\n",
      "step: 37 ... state: [-0.20225367 -0.01000867  0.24649635 -0.03974168] ... action: [ 0.4284826  0.997694  -0.7011034  0.7362521] ... reward: 0.16 ... done: False\n",
      "\n",
      "step: 38 ... state: [-0.1976047   0.00892237  0.29996349 -0.01900196] ... action: [-0.67501414  0.23111913 -0.75236005  0.69601643] ... reward: 0.23 ... done: False\n",
      "\n",
      "step: 39 ... state: [-0.18641326  0.02229541  0.32939836 -0.01390881] ... action: [ 0.6146379   0.13820148 -0.1856334  -0.861666  ] ... reward: 0.18 ... done: False\n",
      "\n",
      "step: 40 ... state: [-0.18453719  0.00367533  0.3062229  -0.04439933] ... action: [ 0.39485756 -0.09291463  0.4441112   0.73276466] ... reward: 0.10 ... done: False\n",
      "\n",
      "step: 41 ... state: [-0.19742368 -0.02579588  0.29754913 -0.04535078] ... action: [ 0.951043    0.7116067  -0.97657186 -0.28004387] ... reward: 0.06 ... done: False\n",
      "\n",
      "step: 42 ... state: [-0.21219555 -0.02962501  0.30212022 -0.05323354] ... action: [ 0.4599811  -0.65674067  0.04207321 -0.89132404] ... reward: 0.03 ... done: False\n",
      "\n",
      "step: 43 ... state: [-0.23616056 -0.04850845  0.28818326 -0.078718  ] ... action: [-0.60000694 -0.9629564   0.5873954  -0.5521506 ] ... reward: 0.00 ... done: False\n",
      "\n",
      "step: 44 ... state: [-0.26090774 -0.04959107  0.27986046 -0.10284402] ... action: [-0.30929664  0.8561626   0.4088288  -0.93632215] ... reward: 0.04 ... done: False\n",
      "\n",
      "step: 45 ... state: [-0.28439698 -0.04774065  0.30843078 -0.0940898 ] ... action: [-0.6706117   0.2429568   0.15445718 -0.5242144 ] ... reward: 0.06 ... done: False\n",
      "\n",
      "step: 46 ... state: [-0.30849427 -0.0482289   0.30696244 -0.11227198] ... action: [0.868428   0.22793192 0.07126561 0.17981996] ... reward: -0.04 ... done: False\n",
      "\n",
      "step: 47 ... state: [-0.3500239  -0.08307266  0.2890943  -0.12031419] ... action: [ 0.46024406 -0.37611002 -0.20355788 -0.5803125 ] ... reward: -0.11 ... done: False\n",
      "\n",
      "step: 48 ... state: [-0.399111   -0.09816025  0.25251168 -0.1515935 ] ... action: [-0.62761396  0.8887448   0.4791016  -0.01908238] ... reward: -0.06 ... done: False\n",
      "\n",
      "step: 49 ... state: [-0.4455983  -0.09374815  0.31029173 -0.04928394] ... action: [-0.5451707  -0.49128702 -0.88394165 -0.13116676] ... reward: 0.08 ... done: False\n",
      "\n",
      "step: 50 ... state: [-0.46710712 -0.04328015  0.34142896 -0.04849005] ... action: [-0.37640825  0.39268696 -0.24449632 -0.64079267] ... reward: 0.13 ... done: False\n",
      "\n",
      "step: 51 ... state: [-0.48298436 -0.03178852  0.36070501 -0.06116512] ... action: [-0.9506425  -0.86550075  0.35878554 -0.09260631] ... reward: 0.14 ... done: False\n",
      "\n",
      "step: 52 ... state: [-0.49191603 -0.01795763  0.34282468 -0.09466083] ... action: [ 0.07315842  0.7933426   0.9806779  -0.56620604] ... reward: 0.05 ... done: False\n",
      "\n",
      "step: 53 ... state: [-0.51890928 -0.05394806  0.34488936 -0.09177715] ... action: [ 0.3261564  -0.47335523 -0.958698    0.5167573 ] ... reward: 0.10 ... done: False\n",
      "\n",
      "step: 54 ... state: [-0.53505522 -0.03229411  0.34191267 -0.1212245 ] ... action: [-0.3599657  -0.2330722   0.17663422  0.6620969 ] ... reward: 0.13 ... done: False\n",
      "\n",
      "step: 55 ... state: [-0.54846114 -0.02681397  0.33551373 -0.14339726] ... action: [ 0.2579637   0.7453013  -0.45291594  0.59609365] ... reward: 0.14 ... done: False\n",
      "\n",
      "step: 56 ... state: [-0.56074768 -0.02435992  0.35837188 -0.15851346] ... action: [-0.6287281   0.9055833   0.37497655 -0.5689846 ] ... reward: 0.02 ... done: False\n",
      "\n",
      "step: 57 ... state: [-0.58809942 -0.05458705  0.31637567 -0.16989294] ... action: [ 0.8947412   0.46171162 -0.49211672 -0.57337606] ... reward: -0.02 ... done: False\n",
      "\n",
      "step: 58 ... state: [-0.62288105 -0.06955498  0.31115524 -0.18012203] ... action: [ 0.03640143 -0.94867456 -0.5850598  -0.15062906] ... reward: 0.02 ... done: False\n",
      "\n",
      "step: 59 ... state: [-0.65057796 -0.05543641  0.287636   -0.20323692] ... action: [-0.25166005 -0.07284915 -0.4447426   0.1735687 ] ... reward: 0.08 ... done: False\n",
      "\n",
      "step: 60 ... state: [-0.67282945 -0.04444705  0.29666224 -0.22489305] ... action: [ 0.7277112  -0.76493627  0.03475821 -0.7358638 ] ... reward: -0.05 ... done: False\n",
      "\n",
      "step: 61 ... state: [-0.71060455 -0.07557114  0.27581852 -0.22766794] ... action: [ 0.43371937 -0.2078806   0.13084263 -0.6334403 ] ... reward: -100.00 ... done: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets play a random episode\n",
    "\n",
    "state = env.reset()\n",
    "done = False\n",
    "step = 0\n",
    "while (not done):\n",
    "    action = env.action_space.sample()\n",
    "    next_state,reward,done,_= env.step(action)\n",
    "    \n",
    "    print (\"step: {} ... state: {} ... action: {} ... reward: {:.2f} ... done: {}\\n\".format(step,state[:4],\n",
    "                                                                                      action,reward,done))  \n",
    "    state = next_state\n",
    "    step+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the Agent with DDPG\n",
    "\n",
    "Run the code cell below to train the agent from scratch.  Alternatively, you can skip to the next code cell to load the pre-trained weights from file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 200 ... Average reward: -107.86 ... Max reward: -21.24\n",
      "Episode 400 ... Average reward: -110.24 ... Max reward: -20.75\n",
      "Episode 405\tAverage Score: -112.75\tScore: -162.52"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-389afb114e55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m#print (state, action, reward, next_state, done)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/learning/rl_bipedal/ddpg_agent.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# Learn, if enough samples are available in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/learning/rl_bipedal/ddpg_agent.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mnext_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_state\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexperiences\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cv3/lib/python3.6/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mvstack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cv3/lib/python3.6/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \"\"\"\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_m\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cv3/lib/python3.6/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36matleast_2d\u001b[0;34m(*arys)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mary\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/cv3/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masanyarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m     \"\"\"\n\u001b[0;32m--> 553\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_episodes=2000\n",
    "max_t=700\n",
    "scores_deque = deque(maxlen=100)\n",
    "scores = []\n",
    "max_score = -np.Inf\n",
    "for i_episode in range(1, n_episodes+1):\n",
    "    state = env.reset()\n",
    "    agent.reset()\n",
    "    score = 0\n",
    "    for t in range(max_t):\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        #print (state, action, reward, next_state, done)\n",
    "        agent.step(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        score += reward\n",
    "        if done:\n",
    "            break \n",
    "    scores_deque.append(score)\n",
    "    scores.append(score)\n",
    "    if max_score<score:\n",
    "        max_score=score\n",
    "    print('\\rEpisode {}\\tAverage Score: {:.2f}\\tScore: {:.2f}'.format(i_episode, np.mean(scores_deque), score), end=\"\")\n",
    "    if i_episode % 200 == 0:\n",
    "        torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "        torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "        print('\\rEpisode {} ... Average reward: {:.2f} ... Max reward: {:.2f}'.format(i_episode,\n",
    "                                                                                      np.mean(scores_deque),max_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Watch a Smart Agent!\n",
    "\n",
    "In the next code cell, you will load the trained weights from file to watch a smart agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to animate a list of frames\n",
    "def animate_frames(frames):\n",
    "    plt.figure(dpi = 72)\n",
    "    plt.axis('off')\n",
    "\n",
    "    # color option for plotting\n",
    "    # use Greys for greyscale\n",
    "    cmap = None if len(frames[0].shape)==3 else 'Greys'\n",
    "    patch = plt.imshow(frames[0], cmap=cmap)  \n",
    "\n",
    "    fanim = animation.FuncAnimation(plt.gcf(), \\\n",
    "        lambda x: patch.set_data(frames[x]), frames = len(frames), interval=30)\n",
    "    \n",
    "    display(display_animation(fanim, default_mode='once'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.actor_local.load_state_dict(torch.load('checkpoint_actor.pth',map_location='cpu'))\n",
    "agent.critic_local.load_state_dict(torch.load('checkpoint_critic.pth',map_location='cpu'))\n",
    "\n",
    "frames = []\n",
    "state = env.reset()\n",
    "agent.reset()\n",
    "for t in range(1000):\n",
    "    action = agent.act(state)\n",
    "    frames.append(env.render(mode='rgb_array')) \n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    state=next_state\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "animate_frames(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv3",
   "language": "python",
   "name": "cv3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
